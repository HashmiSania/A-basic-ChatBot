NLP:

Natural Language Processing (NLP) Defined
Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice. This is also called “language in.” Most consumers have probably interacted with NLP without realizing it. For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa. When we ask questions of these virtual assistants, NLP is what enables them to not only understand the user’s request, but to also respond in natural language. NLP applies both to written text and speech, and can be applied to all human languages. Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking. For example, some email programs can automatically suggest an appropriate reply to a message based on its content—these programs use NLP to read, analyze, and respond to your message.

There are several other terms that are roughly synonymous with NLP. Natural language understanding (NLU) and natural language generation (NLG) refer to using computers to understand and produce human language, respectively. NLG has the ability to provide a verbal description of what has happened. This is also called "language out” by summarizing by meaningful information into text using a concept known as "grammar of graphics."

In practice, NLU is used to mean NLP. The understanding by computers of the structure and meaning of all human languages, allowing developers and users to interact with computers using natural sentences and communication. Computational linguistics (CL) is the scientific field that studies computational aspects of human language, while NLP is the engineering discipline concerned with building computational artifacts that understand, generate, or manipulate human language.

Research on NLP began shortly after the invention of digital computers in the 1950s, and NLP draws on both linguistics and AI. However, the major breakthroughs of the past few years have been powered by machine learning, which is a branch of AI that develops systems that learn and generalize from data. Deep learning is a kind of machine learning that can learn very complex patterns from large datasets, which means that it is ideally suited to learning the complexities of natural language from datasets sourced from the web.

Applications of Natural Language Processing
Automate routine tasks: Chatbots powered by NLP can process a large number of routine tasks that are handled by human agents today, freeing up employees to work on more challenging and interesting tasks. For example, chatbots and Digital Assistants can recognize a wide variety of user requests, match them to the appropriate entry in a corporate database, and formulate an appropriate response to the user.

Improve search: NLP can improve on keyword matching search for document and FAQ retrieval by disambiguating word senses based on context (for example, “carrier” means something different in biomedical and industrial contexts), matching synonyms (for example, retrieving documents mentioning “car” given a search for “automobile”), and taking morphological variation into account (which is important for non-English queries). Effective NLP-powered academic search systems can dramatically improve access to relevant cutting-edge research for doctors, lawyers, and other specialists.

Search engine optimization: NLP is a great tool for getting your business ranked higher in online search by analyzing searches to optimize your content. Search engines use NLP to rank their results—and knowing how to effectively use these techniques makes it easier to be ranked above your competitors. This will lead to greater visibility for your business.

Analyzing and organizing large document collections: NLP techniques such as document clustering and topic modeling simplify the task of understanding the diversity of content in large document collections, such as corporate reports, news articles, or scientific documents. These techniques are often used in legal discovery purposes.

Social media analytics: NLP can analyze customer reviews and social media comments to make better sense of huge volumes of information. Sentiment analysis identifies positive and negative comments in a stream of social-media comments, providing a direct measure of customer sentiment in real time. This can lead to huge payoffs down the line, such as increased customer satisfaction and revenue.

Market insights: With NLP working to analyze the language of your business’ customers, you’ll have a better handle on what they want, and also a better idea of how to communicate with them. Aspect-oriented sentiment analysis detects the sentiment associated with specific aspects or products in social media (for example, “the keyboard is great, but the screen is too dim”), providing directly actionable information for product design and marketing.

Moderating content: If your business attracts large amounts of user or customer comments, NLP enables you to moderate what’s being said in order to maintain quality and civility by analyzing not only the words, but also the tone and intent of comments.

Industries Using Natural Language Processing
NLP simplifies and automates a wide range of business processes, especially ones that involve large amounts of unstructured text like emails, surveys, social media conversations, and more. With NLP, businesses are better able to analyze their data to help make the right decisions. Here are just a few examples of practical applications of NLP:

Healthcare: As healthcare systems all over the world move to electronic medical records, they are encountering large amounts of unstructured data. NLP can be used to analyze and gain new insights into health records.
Legal: To prepare for a case, lawyers must often spend hours examining large collections of documents and searching for material relevant to a specific case. NLP technology can automate the process of legal discovery, cutting down on both time and human error by sifting through large volumes of documents.
Finance: The financial world moves extremely fast, and any competitive advantage is important. In the financial field, traders use NLP technology to automatically mine information from corporate documents and news releases to extract information relevant to their portfolios and trading decisions.
Customer service: Many large companies are using virtual assistants or chatbots to help answer basic customer inquiries and information requests (such as FAQs), passing on complex questions to humans when necessary.
Insurance: Large insurance companies are using NLP to sift through documents and reports related to claims, in an effort to streamline the way business gets done.
NLP Technology Overview
Machine learning models for NLP: We mentioned earlier that modern NLP relies heavily on an approach to AI called machine learning. Machine learning make predictions by generalizing over examples in a dataset. This dataset is called the training data, and machine learning algorithms train on this training data to produce a machine learning model that accomplishes a target task.

For example, sentiment analysis training data consists of sentences together with their sentiment (for example, positive, negative, or neutral sentiment). A machine-learning algorithm reads this dataset and produces a model which takes sentences as input and returns their sentiments. This kind of model, which takes sentences or documents as inputs and returns a label for that input, is called a document classification model. Document classifiers can also be used to classify documents by the topics they mention (for example, as sports, finance, politics, etc.).

Another kind of model is used to recognize and classify entities in documents. For each word in a document, the model predicts whether that word is part of an entity mention, and if so, what kind of entity is involved. For example, in “XYZ Corp shares traded for $28 yesterday”, “XYZ Corp” is a company entity, “$28” is a currency amount, and “yesterday” is a date. The training data for entity recognition is a collection of texts, where each word is labeled with the kinds of entities the word refers to. This kind of model, which produces a label for each word in the input, is called a sequence labeling model.

Sequence to sequence models are a very recent addition to the family of models used in NLP. A sequence to sequence (or seq2seq) model takes an entire sentence or document as input (as in a document classifier) but it produces a sentence or some other sequence (for example, a computer program) as output. (A document classifier only produces a single symbol as output). Example applications of seq2seq models include machine translation, which for example, takes an English sentence as input and returns its French sentence as output; document summarization (where the output is a summary of the input); and semantic parsing (where the input is a query or request in English, and the output is a computer program implementing that request).

Deep learning, pretrained models, and transfer learning: Deep learning is the most widely-used kind of machine learning in NLP. In the 1980s, researchers developed neural networks, in which a large number of primitive machine learning models are combined into a single network: by analogy with brains, the simple machine learning models are sometimes called “neurons.” These neurons are arranged in layers, and a deep neural network is one with many layers. Deep learning is machine learning using deep neural network models.

Because of their complexity, generally it takes a lot of data to train a deep neural network, and processing it takes a lot of compute power and time. Modern deep neural network NLP models are trained from a diverse array of sources, such as all of Wikipedia and data scraped from the web. The training data might be on the order of 10 GB or more in size, and it might take a week or more on a high-performance cluster to train the deep neural network. (Researchers find that training even deeper models from even larger datasets have even higher performance, so currently there is a race to train bigger and bigger models from larger and larger datasets).

The voracious data and compute requirements of Deep Neural Networks would seem to severely limit their usefulness. However, transfer learning enables a trained deep neural network to be further trained to achieve a new task with much less training data and compute effort. The simplest kind of transfer learning is called fine tuning. It consists simply of first training the model on a large generic dataset (for example, Wikipedia) and then further training (“fine-tuning”) the model on a much smaller task-specific dataset that is labeled with the actual target task. Perhaps surprisingly, the fine-tuning datasets can be extremely small, maybe containing only hundreds or even tens of training examples, and fine-tuning training only requires minutes on a single CPU. Transfer learning makes it easy to deploy deep learning models throughout the enterprise.

There is now an entire ecosystem of providers delivering pretrained deep learning models that are trained on different combinations of languages, datasets, and pretraining tasks. These pretrained models can be downloaded and fine-tuned for a wide variety of different target tasks.

Sample of NLP Preprocessing Techniques
Tokenization: Tokenization splits raw text (for example., a sentence or a document) into a sequence of tokens, such as words or subword pieces. Tokenization is often the first step in an NLP processing pipeline. Tokens are commonly recurring sequences of text that are treated as atomic units in later processing. They may be words, subword units called morphemes (for example, prefixes such as “un-“ or suffixes such as “-ing” in English), or even individual characters.

Bag-of-words models: Bag-of-words models treat documents as unordered collections of tokens or words (a bag is like a set, except that it tracks the number of times each element appears). Because they completely ignore word order, bag-of-words models will confuse a sentence such as “dog bites man” with “man bites dog.” However, bag-of-words models are often used for efficiency reasons on large information retrieval tasks such as search engines. They can produce close to state-of-the-art results with longer documents.

Stop word removal: A “stop word” is a token that is ignored in later processing. They are typically short, frequent words such as “a,” “the,” or “an.” Bag-of-words models and search engines often ignore stop words in order to reduce processing time and storage within the database. Deep neural networks typically do take word-order into account (that is, they are not bag-of-words models) and do not do stop word removal because stop words can convey subtle distinctions in meaning (for example, “the package was lost” and “a package is lost” don’t mean the same thing, even though they are the same after stop word removal).

Stemming and lemmatization: Morphemes are the smallest meaning-bearing elements of language. Typically morphemes are smaller than words. For example, “revisited” consists of the prefix “re-“, the stem “visit,” and the past-tense suffix “-ed.” Stemming and lemmatization map words to their stem forms (for example, “revisit” + PAST). Stemming and lemmatization are crucial steps in pre-deep learning models, but deep learning models generally learn these regularities from their training data, and so do not require explicit stemming or lemmatization steps.

Part-of-speech tagging and syntactic parsing: Part-of-speech (PoS) tagging is the process of labeling each word with its part of speech (for example, noun, verb, adjective, etc.). A Syntactic parser identifies how words combine to form phrases, clauses, and entire sentences. PoS tagging is a sequence labeling task, syntactic parsing is an extended kind of sequence labeling task, and deep neural Nntworks are the state-of-the-art technology for both PoS tagging and syntactic parsing. Before deep learning, PoS tagging and syntactic parsing were essential steps in sentence understanding. However, modern deep learning NLP models generally only benefit marginally (if at all) from PoS or syntax information, so neither PoS tagging nor syntactic parsing are widely used in deep learning NLP.

NLP Programming Languages
Python:
The NLP Libraries and toolkits are generally available in Python, and for this reason by far the majority of NLP projects are developed in Python. Python’s interactive development environment makes it easy to develop and test new code.

Java and C++:
For processing large amounts of data, C++ and Java are often preferred because they can support more efficient code.

NLP Libraries and Development Environments
Here are examples of some popular NLP libraries.

TensorFlow and PyTorch: These are the two most popular deep learning toolkits. They are freely available for research and commercial purposes. While they support multiple languages, their primary language is Python. They come with large libraries of prebuilt components, so even very sophisticated deep learning NLP models often only require plugging these components together. They also support high-performance computing infrastructure, such as clusters of machines with graphical processor unit (GPU) accelerators. They have excellent documentation and tutorials.

AllenNLP: This is a library of high-level NLP components (for example, simple chatbots) implemented in PyTorch and Python. The documentation is excellent.

HuggingFace: This company distributes hundreds of different pretrained Deep Learning NLP models, as well as a plug-and-play software toolkit in TensorFlow and PyTorch that enables developers to rapidly evaluate how well different pretrained models perform on their specific tasks.

Spark NLP: Spark NLP is an open source text processing library for advanced NLP for the Python, Java, and Scala programming languages. Its goal is to provide an application programming interface (API) for natural language processing pipelines. It offers pretrained neural network models, pipelines, and embeddings, as well as support for training custom models.

SpaCy NLP: SpaCy is a free, open source library for advanced NLP in Python, and it is specifically designed to help build applications that can process and understand large volumes of text. SpaCy is known to be highly intuitive and can handle many of the tasks needed in common NLP projects.

In summary, Natural language processing is an exciting area of artificial intelligence development that fuels a wide range of new products such as search engines, chatbots, recommendation systems, and speech-to-text systems. As human interfaces with computers continue to move away from buttons, forms, and domain-specific languages, the demand for growth in natural language processing will continue to increase. For this reason, Oracle Cloud Infrastructure is committed to providing on-premises performance with our performance-optimized compute shapes and tools for NLP. Oracle Cloud Infrastructure offers an array of GPU shapes that you can deploy in minutes to begin experimenting with NLP.




ML- Machine Learning

What is ML?
Machine learning (ML) is a branch of artificial intelligence (AI) and computer science that focuses on the using data and algorithms to enable AI to imitate the way that humans learn, gradually improving its accuracy.

How does machine learning work?
UC Berkeley (link resides outside ibm.com) breaks out the learning system of a machine learning algorithm into three main parts.

A Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data.
An Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.
A Model Optimization Process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this iterative “evaluate and optimize” process, updating weights autonomously until a threshold of accuracy has been met. 
Ebook
Unlock Competitive Advantage with Generative AI + ML
Explore the benefits of generative AI and ML and learn how to confidently incorporate these technologies into your business.

Related content
Register for the white paper on AI governance

Machine learning versus deep learning versus neural networks
Since deep learning and machine learning tend to be used interchangeably, it’s worth noting the nuances between the two. Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence. However, neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks.

The way in which deep learning and machine learning differ is in how each algorithm learns. "Deep" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn’t necessarily require a labeled dataset. The deep learning process can ingest unstructured data in its raw form (e.g., text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of large amounts of data. You can think of deep learning as "scalable machine learning" as Lex Fridman notes in this MIT lecture (link resides outside ibm.com).

Classical, or "non-deep," machine learning is more dependent on human intervention to learn. Human experts determine the set of features to understand the differences between data inputs, usually requiring more structured data to learn.

Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. The “deep” in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the input and the output—can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network.

Deep learning and neural networks are credited with accelerating progress in areas such as computer vision, natural language processing, and speech recognition.

See the blog post “AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the Difference?” for a closer look at how the different concepts relate.

Related content
Explore the watsonx.ai interactive demo

Download "Machine learning for Dummies"

- This link downloads a pdf
Explore Gen AI for developers

Machine learning methods
Machine learning models fall into three primary categories.

Supervised machine learning            
Supervised learning, also known as supervised machine learning, is defined by its use of labeled datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, naïve bayes, linear regression, logistic regression, random forest, and support vector machine (SVM).

Unsupervised machine learning
Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets (subsets called clusters). These algorithms discover hidden patterns or data groupings without the need for human intervention. This method’s ability to discover similarities and differences in information make it ideal for exploratory data analysis, cross-selling strategies, customer segmentation, and image and pattern recognition. It’s also used to reduce the number of features in a model through the process of dimensionality reduction. Principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering, and probabilistic clustering methods.

Semi-supervised learning 
Semi-supervised learning offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of not having enough labeled data for a supervised learning algorithm. It also helps if it’s too costly to label enough data. 

For a deep dive into the differences between these approaches, check out "Supervised vs. Unsupervised Learning: What's the Difference?"

Reinforcement machine learning
Reinforcement machine learning is a machine learning model that is similar to supervised learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem.

The IBM Watson® system that won the Jeopardy! challenge in 2011 is a good example. The system used reinforcement learning to learn when to attempt an answer (or question, as it were), which square to select on the board, and how much to wager—especially on daily doubles.

Learn more about reinforcement learning      

Common machine learning algorithms
A number of machine learning algorithms are commonly used. These include:

Neural networks: Neural networks  simulate the way the human brain works, with a huge number of linked processing nodes. Neural networks are good at recognizing patterns and play an important role in applications including natural language translation, image recognition, speech recognition, and image creation.
Linear regression: This algorithm is used to predict numerical values, based on a linear relationship between different values. For example, the technique could be used to predict house prices based on historical data for the area.
Logistic regression: This supervised learning algorithm makes predictions for categorical response variables, such as “yes/no” answers to questions. It can be used for applications such as classifying spam and quality control on a production line.
Clustering: Using unsupervised learning, clustering algorithms can identify patterns in data so that it can be grouped. Computers can help data scientists by identifying differences between data items that humans have overlooked.
Decision trees: Decision trees can be used for both predicting numerical values (regression) and classifying data into categories. Decision trees use a branching sequence of linked decisions that can be represented with a tree diagram. One of the advantages of decision trees is that they are easy to validate and audit, unlike the black box of the neural network.
Random forests: In a random forest, the machine learning algorithm predicts a value or category by combining the results from a number of decision trees.
Advantages and disadvantages of machine learning algorithms 
Depending on your budget, need for speed and precision required, each algorithm type—supervised, unsupervised, semi-supervised, or reinforcement—has its own advantages and disadvantages. For example, decision tree algorithms are used for both predicting numerical values (regression problems) and classifying data into categories. Decision trees use a branching sequence of linked decisions that may be represented with a tree diagram. A prime advantage of decision trees is that they are easier to validate and audit than a neural network. The bad news is that they can be more unstable than other decision predictors. 

Overall, there are many advantages to machine learning that businesses can leverage for new efficiencies. These include machine learning identifying patterns and trends in massive volumes of data that humans might not spot at all. And this analysis requires little human intervention: just feed in the dataset of interest and let the machine learning system assemble and refine its own algorithms—which will continually improve with more data input over time. Customers and users can enjoy a more personalized experience as the model learns more with every experience with that person.

On the downside, machine learning requires large training datasets that are accurate and unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering sufficient data and having a system robust enough to run it might also be a drain on resources. Machine learning can also be prone to error, depending on the input. With too small a sample, the system could produce a perfectly logical algorithm that is completely wrong or misleading. To avoid wasting budget or displeasing customers, organizations should act on the answers only when there is high confidence in the output.

Real-world machine learning use cases
Here are just a few examples of machine learning you might encounter every day:

Speech recognition: It is also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, and it is a capability which uses natural language processing (NLP) to translate human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search—e.g. Siri—or improve accessibility for texting.

Customer service:  Online chatbots are replacing human agents along the customer journey, changing the way we think about customer engagement across websites and social media platforms. Chatbots answer frequently asked questions (FAQs) about topics such as shipping, or provide personalized advice, cross-selling products or suggesting sizes for users. Examples include virtual agents on e-commerce sites; messaging bots, using Slack and Facebook Messenger; and tasks usually done by virtual assistants and voice assistants.

Computer vision: This AI technology enables computers to derive meaningful information from digital images, videos, and other visual inputs, and then take the appropriate action. Powered by convolutional neural networks, computer vision has applications in photo tagging on social media, radiology imaging in healthcare, and self-driving cars in the automotive industry. 

Recommendation engines: Using past consumption behavior data, AI algorithms can help to discover data trends that can be used to develop more effective cross-selling strategies. Recommendation engines are used by online retailers to make relevant product recommendations to customers during the checkout process.

Robotic process automation (RPA): Also known as software robotics, RPA uses intelligent automation technologies to perform repetitive manual tasks.

Automated stock trading: Designed to optimize stock portfolios, AI-driven high-frequency trading platforms make thousands or even millions of trades per day without human intervention.

Fraud detection: Banks and other financial institutions can use machine learning to spot suspicious transactions. Supervised learning can train a model using information about known fraudulent transactions. Anomaly detection can identify transactions that look atypical and deserve further investigation.

Challenges of machine learning
As machine learning technology has developed, it has certainly made our lives easier. However, implementing machine learning in businesses has also raised a number of ethical concerns about AI technologies. Some of these include:

Technological singularity
While this topic garners a lot of public attention, many researchers are not concerned with the idea of AI surpassing human intelligence in the near future. Technological singularity is also referred to as strong AI or superintelligence. Philosopher Nick Bostrum defines superintelligence as “any intellect that vastly outperforms the best human brains in practically every field, including scientific creativity, general wisdom, and social skills.” Despite the fact that superintelligence is not imminent in society, the idea of it raises some interesting questions as we consider the use of autonomous systems, like self-driving cars. It’s unrealistic to think that a driverless car would never have an accident, but who is responsible and liable under those circumstances? Should we still develop autonomous vehicles, or do we limit this technology to semi-autonomous vehicles which help people drive safely? The jury is still out on this, but these are the types of ethical debates that are occurring as new, innovative AI technology develops.

AI impact on jobs
While a lot of public perception of artificial intelligence centers around job losses, this concern should probably be reframed. With every disruptive, new technology, we see that the market demand for specific job roles shifts. For example, when we look at the automotive industry, many manufacturers, like GM, are shifting to focus on electric vehicle production to align with green initiatives. The energy industry isn’t going away, but the source of energy is shifting from a fuel economy to an electric one.

In a similar way, artificial intelligence will shift the demand for jobs to other areas. There will need to be individuals to help manage AI systems. There will still need to be people to address more complex problems within the industries that are most likely to be affected by job demand shifts, such as customer service. The biggest challenge with artificial intelligence and its effect on the job market will be helping people to transition to new roles that are in demand.

Privacy
Privacy tends to be discussed in the context of data privacy, data protection, and data security. These concerns have allowed policymakers to make more strides in recent years. For example, in 2016, GDPR legislation was created to protect the personal data of people in the European Union and European Economic Area, giving individuals more control of their data. In the United States, individual states are developing policies, such as the California Consumer Privacy Act (CCPA), which was introduced in 2018 and requires businesses to inform consumers about the collection of their data. Legislation such as this has forced companies to rethink how they store and use personally identifiable information (PII). As a result, investments in security have become an increasing priority for businesses as they seek to eliminate any vulnerabilities and opportunities for surveillance, hacking, and cyberattacks.

Bias and discrimination
Instances of bias and discrimination across a number of machine learning systems have raised many ethical questions regarding the use of artificial intelligence. How can we safeguard against bias and discrimination when the training data itself may be generated by biased human processes? While companies typically have good intentions for their automation efforts, Reuters (link resides outside ibm.com) highlights some of the unforeseen consequences of incorporating AI into hiring practices. In their effort to automate and simplify a process, Amazon unintentionally discriminated against job candidates by gender for technical roles, and the company ultimately had to scrap the project. Harvard Business Review (link resides outside ibm.com) has raised other pointed questions about the use of AI in hiring practices, such as what data you should be able to use when evaluating a candidate for a role.

Bias and discrimination aren’t limited to the human resources function either; they can be found in a number of applications from facial recognition software to social media algorithms.

As businesses become more aware of the risks with AI, they’ve also become more active in this discussion around AI ethics and values. For example, IBM has sunset its general purpose facial recognition and analysis products. IBM CEO Arvind Krishna wrote: “IBM firmly opposes and will not condone uses of any technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and Principles of Trust and Transparency.”

Accountability
Since there isn’t significant legislation to regulate AI practices, there is no real enforcement mechanism to ensure that ethical AI is practiced. The current incentives for companies to be ethical are the negative repercussions of an unethical AI system on the bottom line. To fill the gap, ethical frameworks have emerged as part of a collaboration between ethicists and researchers to govern the construction and distribution of AI models within society. However, at the moment, these only serve to guide. Some research (link resides outside ibm.com) shows that the combination of distributed responsibility and a lack of foresight into potential consequences aren’t conducive to preventing harm to society.

Read more about IBM's position on AI Ethics

How to choose the right AI platform for machine learning 
Selecting a platform can be a challenging process, as the wrong system can drive up costs, or limit the use of other valuable tools or technologies. When reviewing multiple vendors to select an AI platform, there is often a tendency to think that more features = a better system. Maybe so, but reviewers should start by thinking through what the AI platform will be doing for their organization. What machine learning capabilities need to be delivered and what features are important to accomplish them? One missing feature might doom the usefulness of an entire system. Here are some features to consider.

MLOps capabilities. Does the system have:
a unified interface for ease of management?
automated machine learning tools for faster model creation with low-code and no-code functionality?
decision optimization to streamline the selection and deployment of optimization models?
visual modeling to combine visual data science with open-source libraries and notebook-based interfaces on a unified data and AI studio?
automated development for beginners to get started quickly and more advanced data scientists to experiment?
synthetic data generator as an alternative or supplement to real-world data when real-world data is not readily available?
 
Generative AI capabilities. Does the system have:
a content generator that can generate text, images and other content based on the data it was trained on?
automated classification to read and classify written input, such as evaluating and sorting customer complaints or reviewing customer feedback sentiment?
a summary generator that can transform dense text into a high-quality summary, capture key points from financial reports, and generate meeting transcriptions?
a data extraction capability to sort through complex details and quickly pull the necessary information from large documents?

